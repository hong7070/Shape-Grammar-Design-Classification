# Attributing Meaning to Shape Grammar Generated Designs: A Machine Learning Approach

**Authors:** Seunghwan Hong, Chowdhury Ali Imam

## Project Goal

This project explores the use of machine learning techniques to classify and understand architectural designs generated by shape grammars based on subjective human perceptions. The primary objective is to investigate whether ML models can learn to associate sequences of shape grammar rules with qualitative design labels (e.g., "Monumental," "Chaos," "Dynamic"), potentially incorporating annotator confidence to improve prediction accuracy. An alternative clustering-based approach using physical design metrics is also explored.

## Background: Shape Grammars in Design

Shape grammars provide a formal system for generating designs through the application of rules. A set of initial shapes is transformed by recursively applying these rules, each of which defines how a shape can be altered or replaced. This project utilizes a specific shape grammar capable of generating a diverse range of 3D voxel-based architectural forms from sequences of rules. The challenge lies in the fact that these generated designs, defined by their rule sequences, lack inherent semantic meaning from a human subjective standpoint.

*(For further details on the specific shape grammar rules and generation process, the underlying research can be referenced at [https://github.com/Chowdhury-Ali-Imam/3D-Voxel-Grammar-Implementation](https://github.com/Chowdhury-Ali-Imam/3D-Voxel-Grammar-Implementation) as indicated in the original project context.)*

## Methodology

### 1. Data Generation and Collection

* **Design Generation:** 400 unique rule sequences, each with a maximum length of 20 rules (combining massing and void-creating rules), were used to generate 3D voxel models and corresponding 2D images using Rhino 3D.
* **Survey Data Collection (`process_responses.py`):**
    * Human annotators were surveyed (originally via a Google Forms system) to label these designs.
    * For each design, annotators selected one of six subjective labels: **Monumental, Chaos, Dynamic, Formal, Intimate, Balanced**.
    * Annotators also provided a **confidence score (0-10)** for their choice.
    * Due to data collection challenges (e.g., concurrency issues), the final cleaned dataset comprised approximately 220 valid, labeled design instances.
    * The `process_responses.py` script is responsible for parsing the raw survey data, matching it with design rule sequences from a `design_sequence.csv` (not included in this repository but was part of the original project workflow), and formatting it into `processed_responses.csv`.

### 2. Data Preprocessing (`prepare_ml_data.py`)

* **Rule Sequence Encoding:**
    * Rule sequences (up to 20 characters) were zero-padded to ensure a consistent fixed length of 20.
    * These 20-character sequences were then one-hot encoded into 180-dimensional vectors. Each of the 20 positions in a sequence was treated as a categorical feature, with 9 possible unique tokens (digits '0'-'4' for massing/padding, letters 'A'-'D' for court/padding).
* **Label Encoding:** The six categorical subjective labels were one-hot encoded.
* **Feature Sets Generated:**
    * `X_wo_conf.csv`: Contains only the one-hot encoded rule sequences.
    * `X_with_conf.csv`: Contains the one-hot encoded rule sequences plus the annotator's confidence score as an additional feature.
    * `y.csv`: Contains the one-hot encoded labels.
* The `prepare_ml_data.py` script takes `processed_responses.csv` (output of `process_responses.py`) as input to generate these final feature and label CSV files.

### 3. Machine Learning Experiments

Various machine learning models were trained and evaluated to classify the design rule sequences based on the subjective labels. The impact of annotator confidence was explored through three configurations:
1.  **Without Confidence:** Models trained solely on one-hot encoded rule sequences.
2.  **Confidence as Feature:** Annotator confidence scores included as an explicit input feature alongside the rule sequences.
3.  **Confidence as Sample Weight:** Annotator confidence scores (normalized) used to weight the importance of samples during model training.

**Models Trained:**

* **Decision Tree & Random Forest (`train_tree.py`):**
    * Standard tree-based classifiers from scikit-learn.
    * Random Forest used `n_estimators=100`.
    * Evaluated under all three confidence configurations.
* **1D Convolutional Neural Network (CNN) (`train_cnn.py`):**
    * Implemented using TensorFlow/Keras.
    * **Architecture (Sequence Only):** Input (20 positions, 9 channels) -> Conv1D (32 filters, kernel 3, ReLU) -> MaxPooling1D -> Conv1D (64 filters, kernel 3, ReLU) -> MaxPooling1D -> Flatten -> Dense (100, ReLU) -> Dense (6 classes, Softmax).
    * **Architecture (Sequence + Confidence Feature):** Similar to above, but the confidence score is concatenated with the flattened CNN output before a final Dense layer (50 neurons, ReLU), then the Softmax output layer.
    * Training: Adam optimizer, categorical cross-entropy loss, 100 epochs, batch size 32, 10% validation split.
    * Evaluated under all three confidence configurations.

### 4. Evaluation

* **Primary Metric:** Accuracy (Correct Predictions / Total Predictions).
* **Secondary Metrics:** Per-class precision, recall, and F1-scores, particularly important given the small dataset size and potential class imbalances.
* Results were typically evaluated on a 70/30 train/test split of the ~220 valid samples.

## Results Summary

Detailed outputs and logs can be found in `tree_results.txt` (for tree-based models) and `cnn_results.txt` (for CNNs).

**Key Accuracy Findings:**

| Model               | Configuration                     | Accuracy      |
|---------------------|-----------------------------------|---------------|
| Decision Tree       | Without confidence                | 22.39%        |
| Decision Tree       | With confidence as feature        | 23.88%        |
| Decision Tree       | With sequence + sample weights    | 22.39%        |
| Random Forest       | Without confidence                | 16.42%        |
| Random Forest       | With confidence as feature        | 22.39%        |
| Random Forest       | With sequence + sample weights    | 19.40%        |
| CNN                 | Without confidence                | 22.39%        |
| CNN                 | With confidence as feature        | 25.37%        |
| **CNN** | **With sequence + sample weights**| **28.36%** |

**Observations:**
* The CNN model using confidence scores as sample weights achieved the highest accuracy (28.36%).
* Incorporating confidence scores generally provided a modest benefit, with the optimal method varying by model type.
* Random Forest performance improved significantly when confidence was used as a feature.
* CNNs exhibited signs of overfitting on the small dataset (training accuracies approached 1.0 while validation accuracies remained low and unstable, as seen in `cnn_results.txt`).
* Per-class metrics (detailed in `tree_results.txt` and `cnn_results.txt`) were often low for specific classes, indicating challenges with class imbalance or the distinctiveness of features for certain subjective labels.

## Discussion

The experiments indicate that classifying architectural designs based on subjective labels derived from rule sequences is a highly challenging task. The peak accuracy of 28.36% is only moderately above random chance for a 6-class problem. This difficulty likely stems from:
1.  **High Subjectivity:** Labels like "Monumental" are inherently open to varied interpretations.
2.  **Small Dataset Size:** ~220 samples is insufficient for robustly training complex models, especially for a multi-class problem with high-dimensional input, leading to overfitting.
3.  **Representation Complexity:** One-hot encoded rule sequences may not directly capture the holistic visual properties that drive human subjective perception.

### Alternative Approach Explored (Conceptual)
The original project also considered an alternative:
* Clustering 3D models using quantifiable physical metrics (e.g., volume, surface area, compactness, anisotropy).
* Applying K-means clustering (e.g., k=6).
* Allowing human designers to then interpret and assign their own semantic meaning to these physically-derived clusters.
This alternative aimed to mitigate label ambiguity by grounding clusters in objective measures first, then layering subjective interpretation. This suggested that physically distinct forms could evoke similar emotions, or emotions not covered by the initial fixed label set.

## Code Structure & Files

* **Python Scripts:**
    * `process_responses.py`: Parses raw survey data and aligns with design rule sequences. (Requires `design_sequence.csv` and raw survey CSV as external inputs not in this repo).
    * `prepare_ml_data.py`: Performs one-hot encoding and generates feature/label CSVs from the output of `process_responses.py`.
    * `train_tree.py`: Trains and evaluates Decision Tree and Random Forest models.
    * `train_cnn.py`: Trains and evaluates CNN models.
* **Data Files (Generated by `prepare_ml_data.py`):**
    * `data/X_wo_conf.csv`: Feature set: One-hot encoded rule sequences.
    * `data/X_with_conf.csv`: Feature set: One-hot encoded rule sequences + confidence scores.
    * `data/y.csv`: Labels: One-hot encoded subjective design categories.
* **Results Files:**
    * `results/model_results.txt`: Accuracy and classification reports for Decision Tree, Random Forest (and historically Logistic Regression).
    * `results/cnn_results.txt`: Detailed epoch-by-epoch training log and final evaluation for CNN models.

## How to Run the Code

1.  **Prerequisites:**
    * Python 3.x
    * Pandas
    * Scikit-learn
    * TensorFlow
    * NumPy

2.  **Data Preparation:**
    * The provided `X_wo_conf.csv`, `X_with_conf.csv`, and `y.csv` (ideally placed in a `data/` subdirectory) are the direct inputs for the training scripts.
    * To regenerate these, you would need the original `design_sequence.csv` and the raw survey response CSV. Then:
        1.  Run `python process_responses.py` (this creates `processed_responses.csv`).
        2.  Run `python prepare_ml_data.py` (this uses `processed_responses.csv` to create the `X_*.csv` and `y.csv` files).
        *(Note: The `BASE_DIR` in the scripts assumes they are run from the directory where they reside, and that input CSVs for `prepare_ml_data.py` and `process_responses.py` are in that same directory or handled by their internal path logic.)*

3.  **Model Training and Evaluation:**
    * For Decision Tree and Random Forest models:
        ```bash
        python train_tree.py
        ```
        (This script loads data from `X_wo_conf.csv`, `X_with_conf.csv`, and `y.csv` assumed to be in the same directory, and prints results to the console. These results are also captured in `results/tree_results.txt`.)
    * For CNN models:
        ```bash
        python train_cnn.py
        ```
        (This script loads data similarly and prints epoch-wise training progress and final results to the console. These are also captured in `results/cnn_results.txt`.)

## Future Work
* Acquire a significantly larger and more consistently labeled dataset to improve model robustness and reduce overfitting.
* Further investigate the use of physical/geometric features of the 3D designs as input to ML models for predicting subjective labels.
* Develop an interactive system for design retrieval based on subjective queries, leveraging the insights gained.

## Statement of Contribution
* **Seunghwan Hong:** Primary development of the survey system, data processing pipeline (`process_responses.py`, `prepare_ml_data.py`), machine learning data preparation, and implementation, training, and evaluation of Decision Tree, Random Forest, and CNN models (`train_tree.py`, `train_cnn.py`).
* **Chowdhury Ali Imam:** Conceptualization of the research problem, literature review, background study on shape grammars, design and execution of the alternative clustering approach, initial data generation (rule sequences, designs), and model training/evaluation for Logistic Regression.
